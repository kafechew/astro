# Detailed RAG Design for hermitAI

This document refines the RAG (Retrieval Augmented Generation) design outlined in Section 2 of `mongo.md`, focusing on the data model, ingestion pipeline, vector search setup, and integration into the existing AI chat flow.

**1. Data Model for `knowledge_documents` (MongoDB Collection)**

The proposed schema in `mongo.md` is largely suitable. We'll refine it slightly and add specifics:

*   **Collection Name:** `knowledge_documents`
*   **Schema:**
    *   `_id`: `ObjectId` (Primary Key, automatically generated by MongoDB)
    *   `userId`: `ObjectId` (Indexed, Foreign Key to `users._id`. Ensures data isolation per user.)
    *   `sourceUrl`: `String` (Optional. URL if the source was a webpage.)
    *   `sourceType`: `String` (Enum: "pdf", "txt", "md", "url", "text". Indicates the origin type of the content.)
    *   `originalFilename`: `String` (Optional. Original filename for uploaded files.)
    *   `title`: `String` (Optional. User-provided title for "text" input, or extracted title from documents/webpages if possible.)
    *   `content`: `String` (The text content of the chunk.)
    *   `chunkId`: `String` (UUID. A unique identifier for this specific chunk within its original source document. Useful for referencing or re-assembling, though re-assembly is not a primary goal for RAG context.)
    *   `embedding`: `Array` of `Float` (Vector embedding of the `content`. This field will be indexed for vector search.)
    *   `metadata`: `Object` (Stores contextual information about the chunk.)
        *   `pageNumber`: `Number` (For PDF chunks, the page number where the chunk originated.)
        *   `sectionTitle`: `String` (If identifiable, the section title from which the chunk was extracted.)
        *   `originalSourceId`: `String` (Optional, could be a hash of the original file or a unique ID assigned to the source document before chunking, to group chunks from the same source.)
        *   `chunkOrder`: `Number` (The sequential order of this chunk within the original document.)
    *   `createdAt`: `Date` (Timestamp of document creation.)
    *   `updatedAt`: `Date` (Timestamp of last document update.)

*   **Embedding Generation Strategy:**
    *   **Model:** Google Vertex AI `textembedding-gecko@003` (or the latest stable equivalent). This model provides a good balance of performance and cost.
    *   **Dimensions:** `textembedding-gecko@003` produces 768-dimensional embeddings. The `embedding` field should store these.

**2. Data Ingestion Pipeline - Detailed Steps & API Design**

The ingestion pipeline will handle various input types, process them into chunks, generate embeddings, and store them. All ingestion API endpoints will be protected and require user authentication.

*   **2.1. Source Input & API Endpoints:**

    *   **A. File Uploads (PDF, TXT, MD):**
        *   **API Endpoint:** `POST /api/rag/ingest/upload` (Protected)
        *   **Request:** `multipart/form-data` containing the file. The form field name for the file should be consistent (e.g., `documentFile`).
        *   **Processing:**
            1.  **Authentication & Authorization:** Verify user session.
            2.  **File Reception:** Receive and temporarily store the uploaded file.
            3.  **Text Extraction:**
                *   PDF: Use `pdf-parse` library.
                *   TXT/MD: Direct read of file content.
            4.  **Proceed to Common Processing (2.2).**

    *   **B. URL Submissions (Web Content Scraping):**
        *   **API Endpoint:** `POST /api/rag/ingest/url` (Protected)
        *   **Request:** JSON body: `{ "url": "https://example.com/article" }`
        *   **Processing:**
            1.  **Authentication & Authorization:** Verify user session.
            2.  **URL Validation:** Basic validation of the URL format.
            3.  **Content Fetching:**
                *   Utilize the BrightData MCP server's `scrape_as_markdown` tool. This is preferred for cleaner text.
                *   If `scrape_as_markdown` fails or is unsuitable, `scrape_as_html` can be used, followed by an HTML-to-text conversion library (e.g., `html-to-text`) to clean the content.
            4.  **Proceed to Common Processing (2.2).**

    *   **C. Direct Text Input:**
        *   **API Endpoint:** `POST /api/rag/ingest/text` (Protected)
        *   **Request:** JSON body: `{ "title": "My Notes on X", "text": "This is a long piece of text..." }`
        *   **Processing:**
            1.  **Authentication & Authorization:** Verify user session.
            2.  **Input Validation:** Ensure `text` field is present and not empty.
            3.  **Proceed to Common Processing (2.2).**

*   **2.2. Common Processing Steps (for all inputs):**

    1.  **Chunking Strategy:**
        *   **Method:** Recursive character text splitter. This method tries to keep paragraphs/sentences together.
        *   **Chunk Size:** Target ~500-1000 tokens (approximately 2000-4000 characters, but token count is more accurate for LLMs). This needs to be configurable and tested. Vertex AI `textembedding-gecko` models have input token limits (e.g., 3072 tokens for `textembedding-gecko@003`). Chunks must be smaller than this.
        *   **Chunk Overlap:** ~50-100 tokens. This helps maintain context between chunks.
        *   **Library:** Consider using a library like `langchain` (specifically its text splitters) or `recursive-character-text-splitter` if a standalone solution is preferred.
        *   **Output:** An array of text chunks.

    2.  **Embedding Generation:**
        *   For each text chunk:
            *   Call the Vertex AI `textembedding-gecko@003` model (via `src/services/vertexAiService.js` or a similar dedicated service) to get its 768-dimension embedding.
            *   Handle potential API errors from Vertex AI (e.g., rate limits, invalid input).

    3.  **Storage:**
        *   For each chunk and its embedding:
            *   Construct a document according to the `knowledge_documents` schema defined in Section 1.
            *   Include `userId`, `sourceType`, `originalFilename` (if applicable), `sourceUrl` (if applicable), `title` (if applicable), `chunkId` (generate a new UUID for each chunk), `content` (the chunk text), `embedding`, and relevant `metadata` (page number, chunk order, etc.).
            *   Perform a bulk insert or individual inserts into the `knowledge_documents` MongoDB collection.

*   **2.3. User Feedback on Ingestion:**

    *   **API Response:**
        *   **Success:** Return a `202 Accepted` status if the ingestion is an asynchronous background process, or `201 Created` if processed synchronously and successfully. The response body could include:
            *   A job ID if asynchronous (for future status checks, though not planned for the initial phase).
            *   A summary: e.g., `{ "message": "Document processed successfully.", "sourceId": "...", "chunksCreated": 15 }`.
        *   **Error:**
            *   `400 Bad Request`: For invalid input (e.g., bad URL, missing file, empty text).
            *   `401 Unauthorized`: If user is not authenticated.
            *   `413 Payload Too Large`: If uploaded file exceeds size limits.
            *   `500 Internal Server Error`: For unexpected errors during processing (e.g., text extraction failure, embedding generation failure, database error).
            *   The error response body should contain a clear `error` code and `message`.
    *   **UI Feedback:** No immediate complex UI for ingestion status tracking is planned for this phase. The API response will be the primary feedback mechanism. The client can display success/error messages based on this.

**3. Vector Search Index (MongoDB Atlas)**

*   **Index Name:** A descriptive name, e.g., `idx_vector_embedding_content`
*   **Field to Index:** `embedding` (the array of floats)
*   **Index Type:** **`vectorSearch` index type in Atlas.**
    *   Within the `vectorSearch` index definition, Atlas offers methods like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index). **HNSW** is generally recommended for a good balance of speed and accuracy for many use cases. This should be the default choice.
    *   Configuration for HNSW:
        *   `numDimensions`: 768 (matching the `textembedding-gecko@003` output)
        *   `similarity`: **`cosine`** (common and effective for text embeddings)
        *   `m`: (Max connections per layer, default 16 is often fine, can be tuned)
        *   `efConstruction`: (Size of dynamic list for candidates during construction, default 100, can be tuned)
*   **Creation:** This index needs to be created in the MongoDB Atlas UI, via the Atlas Admin API, or using a provisioning script (e.g., using `mongosh` or a Node.js script with the MongoDB driver) that defines the index. It's not automatically created by the application code merely by defining the schema.
*   **Filtering:** The vector search query will also need to filter by `userId` to ensure data privacy. This is done in the query itself, not typically as part of the vector index definition directly, but the `userId` field should be regularly indexed for efficient filtering.

**4. Vector Search Query Integration into `src/pages/api/ai/chat.js`**

The RAG logic will be integrated into the existing chat API endpoint to augment LLM queries with relevant context from the user's `knowledge_documents`.

*   **Trigger:**
    *   RAG will be **always on** for authenticated users who have ingested documents. If a user has no documents in `knowledge_documents`, the vector search step will simply return no results, and the chat will proceed as normal without augmented context.

*   **Location in `src/pages/api/ai/chat.js`:**
    *   The RAG steps should occur *after* user authentication, email verification, and credit checks.
    *   It should occur *before* the existing logic that calls `NODE_AGENT_SERVICE_URL` or falls back to the in-process ReAct logic.

*   **Detailed Steps within `chat.js`:**

    1.  **Embed User Query:**
        *   Take the incoming `userMessage`.
        *   Generate an embedding for `userMessage` using the same Vertex AI model (`textembedding-gecko@003`) and service used for document ingestion.
        *   `const userQueryEmbedding = await getVertexEmbedding(userMessage);`

    2.  **Perform Vector Search:**
        *   Construct a MongoDB aggregation pipeline using the `$vectorSearch` stage.
        *   `const { db } = await connectToDatabase();`
        *   `const knowledgeCollection = db.collection('knowledge_documents');`
        *   `const relevantChunks = await knowledgeCollection.aggregate([`
            *   `  {`
            *   `    $vectorSearch: {`
            *   `      index: "idx_vector_embedding_content", // Your Atlas Vector Search index name`
            *   `      path: "embedding", // Field containing the vector`
            *   `      queryVector: userQueryEmbedding,`
            *   `      numCandidates: 100, // Number of candidates to consider (tunable)`
            *   `      limit: 3, // Number of top results to return (tunable, e.g., 3-5)`
            *   `      filter: { userId: new ObjectId(user.userId) } // CRITICAL: Filter by userId`
            *   `    }`
            *   `  },`
            *   `  {`
            *   `    $project: { // Optionally project only necessary fields`
            *   `      _id: 0,`
            *   `      content: 1,`
            *   `      sourceUrl: 1,`
            *   `      originalFilename: 1,`
            *   `      title: 1,`
            *   `      score: { $meta: "vectorSearchScore" } // Include search score if needed for ranking/thresholding`
            *   `    }`
            *   `  }`
            *   `]).toArray();`

    3.  **Context Augmentation:**
        *   If `relevantChunks` are found:
            *   Concatenate the `content` of these chunks into a single context string.
            *   `const contextString = relevantChunks.map(chunk => chunk.content).join("\\n\\n---\\n\\n");`
            *   Consider adding source information to the context if desired, e.g., `Source: ${chunk.title || chunk.originalFilename || chunk.sourceUrl}`.

    4.  **Modified LLM Prompt / Agent Input:**
        *   If `contextString` is not empty:
            *   Prepend or append this context to the user's original query when forming the input for the LLM (either the `NODE_AGENT_SERVICE_URL` or the in-process ReAct logic).
            *   **Example for `NODE_AGENT_SERVICE_URL`:**
                *   The body sent to the agent service could be modified:
                    `{ message: userMessage, rag_context: contextString }`
                    (The agent service would then need to be updated to handle this `rag_context`).
            *   **Example for in-process ReAct logic:**
                *   Modify `firstPassPrompt` (or the prompt sent directly to `getVertexAiResponse` if no tools are used):
                    `let augmentedUserMessage = userMessage;`
                    `if (contextString) {`
                    `  augmentedUserMessage = "Use the following context to answer the user's query:\\n\\nContext:\\n\"\"\"\\n" + contextString + "\\n\"\"\"\\n\\nUser Query: " + userMessage;`
                    `}`
                    `// ... then use augmentedUserMessage in prompts`
        *   The prompt should clearly instruct the LLM to prioritize the provided context if relevant.

    5.  **Proceed with existing chat logic:** The rest of the chat flow (agent call or ReAct loop, response streaming, credit update in JWT) continues, now potentially using the augmented context.

**5. Dependencies (Node.js)**

Reconfirming and adding necessary dependencies to `package.json`:

*   `mongodb`: (Already listed, official MongoDB driver)
*   `@google-cloud/aiplatform`: (Already listed, for Vertex AI embeddings and LLM)
*   `pdf-parse`: (Already listed, for PDF text extraction)
*   `uuid`: For generating `chunkId`s (e.g., `npm install uuid` and `import { v4 as uuidv4 } from 'uuid';`)
*   Optional (Chunking):
    *   `langchain`: If using its text splitters. (This is a larger dependency).
    *   `recursive-character-text-splitter`: A more lightweight, focused library for this specific chunking strategy.
*   Optional (HTML Cleaning if `scrape_as_html` is used as fallback):
    *   `html-to-text`: For converting HTML from scraping to clean text.