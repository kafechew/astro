# Detailed RAG Design for hermitAI

This document refines the RAG (Retrieval Augmented Generation) design outlined in Section 2 of `mongo.md`, focusing on the data model, ingestion pipeline, vector search setup, and integration into the existing AI chat flow.

**1. Data Model for `knowledge_documents` (MongoDB Collection)**

The proposed schema in `mongo.md` is largely suitable. We'll refine it slightly and add specifics:

*   **Collection Name:** `knowledge_documents`
*   **Schema:**
    *   `_id`: `ObjectId` (Primary Key, automatically generated by MongoDB)
    *   `userId`: `ObjectId` (Indexed, Foreign Key to `users._id`. Ensures data isolation per user.)
    *   `sourceUrl`: `String` (Optional. URL if the source was a webpage.)
    *   `sourceType`: `String` (Enum: "pdf", "txt", "md", "url", "text". Indicates the origin type of the content.)
    *   `originalFilename`: `String` (Optional. Original filename for uploaded files.)
    *   `title`: `String` (Optional. User-provided title for "text" input, or extracted title from documents/webpages if possible.)
    *   `content`: `String` (The text content of the chunk.)
    *   `chunkId`: `String` (UUID. A unique identifier for this specific chunk within its original source document. Useful for referencing or re-assembling, though re-assembly is not a primary goal for RAG context.)
    *   `embedding`: `Array` of `Float` (Vector embedding of the `content`. This field will be indexed for vector search.)
    *   `metadata`: `Object` (Stores contextual information about the chunk.)
        *   `pageNumber`: `Number` (For PDF chunks, the page number where the chunk originated.)
        *   `sectionTitle`: `String` (If identifiable, the section title from which the chunk was extracted.)
        *   `originalSourceId`: `String` (Optional, could be a hash of the original file or a unique ID assigned to the source document before chunking, to group chunks from the same source.)
        *   `chunkOrder`: `Number` (The sequential order of this chunk within the original document.)
    *   `createdAt`: `Date` (Timestamp of document creation.)
    *   `updatedAt`: `Date` (Timestamp of last document update.)

*   **Embedding Generation Strategy:**
    *   **Model:** `@google/generative-ai` library, specifically the `models/text-embedding-004` model.
    *   **Dimensions:** `models/text-embedding-004` produces 768-dimensional embeddings.
    *   **Function:** Embeddings are generated using the `getEmbeddingForQuery(text, taskType)` function in `src/services/ragService.js`.
        *   `taskType: "RETRIEVAL_DOCUMENT"` is used for ingesting and chunking documents.
        *   `taskType: "RETRIEVAL_QUERY"` is used for embedding user queries at search time.

**2. Data Ingestion Pipeline - Detailed Steps & API Design**

The ingestion pipeline will handle various input types, process them into chunks, generate embeddings, and store them. All ingestion API endpoints will be protected and require user authentication.

*   **2.1. Source Input & API Endpoints:**

    *   **A. File Uploads (PDF, TXT, MD):**
        *   **API Endpoint:** `POST /api/rag/ingest/upload` (Protected)
        *   **Request:** `multipart/form-data` containing the file. The form field name for the file should be consistent (e.g., `documentFile`).
        *   **Processing:**
            1.  **Authentication & Authorization:** Verify user session.
            2.  **File Reception:** Receive and temporarily store the uploaded file.
            3.  **Text Extraction:**
                *   PDF: Use `pdf-parse` library.
                *   TXT/MD: Direct read of file content.
            4.  **Proceed to Common Processing (2.2).**

    *   **B. URL Submissions (Web Content Scraping):**
        *   **API Endpoint:** `POST /api/rag/ingest/url` (Protected)
        *   **Request:** JSON body: `{ "url": "https://example.com/article" }`
        *   **Processing:**
            1.  **Authentication & Authorization:** Verify user session.
            2.  **URL Validation:** Basic validation of the URL format.
            3.  **Content Fetching:**
                *   Utilize the BrightData MCP server's `scrape_as_markdown` tool. This is preferred for cleaner text.
                *   If `scrape_as_markdown` fails or is unsuitable, `scrape_as_html` can be used, followed by an HTML-to-text conversion library (e.g., `html-to-text`) to clean the content.
            4.  **Proceed to Common Processing (2.2).**

    *   **C. Direct Text Input:**
        *   **API Endpoint:** `POST /api/rag/ingest/text` (Protected)
        *   **Request:** JSON body: `{ "title": "My Notes on X", "text": "This is a long piece of text..." }`
        *   **Processing:**
            1.  **Authentication & Authorization:** Verify user session.
            2.  **Input Validation:** Ensure `text` field is present and not empty.
            3.  **Proceed to Common Processing (2.2).**

*   **2.2. Common Processing Steps (for all inputs):**

    1.  **Chunking Strategy:**
        *   **Method:** Recursive character text splitter. This method tries to keep paragraphs/sentences together.
        *   **Chunk Size:** Target ~500-1000 tokens (approximately 2000-4000 characters, but token count is more accurate for LLMs). This needs to be configurable and tested. Vertex AI `textembedding-gecko` models have input token limits (e.g., 3072 tokens for `textembedding-gecko@003`). Chunks must be smaller than this.
        *   **Chunk Overlap:** ~50-100 tokens. This helps maintain context between chunks.
        *   **Library:** Consider using a library like `langchain` (specifically its text splitters) or `recursive-character-text-splitter` if a standalone solution is preferred.
        *   **Output:** An array of text chunks.

    2.  **Embedding Generation:**
        *   For each text chunk:
            *   Call the `getEmbeddingForQuery(chunkText, "RETRIEVAL_DOCUMENT")` function from `src/services/ragService.js` to get its 768-dimension embedding using `@google/generative-ai`'s `models/text-embedding-004`.
            *   Handle potential API errors (e.g., rate limits, invalid input).

    3.  **Storage:**
        *   For each chunk and its embedding:
            *   Construct a document according to the `knowledge_documents` schema defined in Section 1.
            *   Include `userId`, `sourceType`, `originalFilename` (if applicable), `sourceUrl` (if applicable), `title` (if applicable), `chunkId` (generate a new UUID for each chunk), `content` (the chunk text), `embedding`, and relevant `metadata` (page number, chunk order, etc.).
            *   Perform a bulk insert or individual inserts into the `knowledge_documents` MongoDB collection.

*   **2.3. User Feedback on Ingestion:**

    *   **API Response:**
        *   **Success:** Return a `202 Accepted` status if the ingestion is an asynchronous background process, or `201 Created` if processed synchronously and successfully. The response body could include:
            *   A job ID if asynchronous (for future status checks, though not planned for the initial phase).
            *   A summary: e.g., `{ "message": "Document processed successfully.", "sourceId": "...", "chunksCreated": 15 }`.
        *   **Error:**
            *   `400 Bad Request`: For invalid input (e.g., bad URL, missing file, empty text).
            *   `401 Unauthorized`: If user is not authenticated.
            *   `413 Payload Too Large`: If uploaded file exceeds size limits.
            *   `500 Internal Server Error`: For unexpected errors during processing (e.g., text extraction failure, embedding generation failure, database error).
            *   The error response body should contain a clear `error` code and `message`.
    *   **UI Feedback:** No immediate complex UI for ingestion status tracking is planned for this phase. The API response will be the primary feedback mechanism. The client can display success/error messages based on this.

**3. Vector Search Index (MongoDB Atlas)**

*   **Index Name:** `vector_index_knowledge_cosine` (This is the exact name used in `src/services/ragService.js` and should be stored in `process.env.VECTOR_SEARCH_INDEX_NAME`).
*   **Field to Index:** `embedding` (the array of floats)
*   **Index Type:** **`vectorSearch` index type in Atlas.**
    *   Within the `vectorSearch` index definition:
        *   `numDimensions`: 768 (matching the `models/text-embedding-004` output)
        *   `similarity`: **`cosine`**
        *   Atlas offers methods like HNSW or IVF. HNSW is generally recommended.
*   **Creation:** This index needs to be **manually created** in the MongoDB Atlas UI for the `knowledge_documents` collection. The `userId` field must be mapped as type `filter` in the index definition for it to be usable in the `$vectorSearch` stage's `filter` option.

**4. RAG Logic and Integration (`src/services/ragService.js` and `src/pages/api/ai/chat.js`)**

The RAG logic is primarily encapsulated in `src/services/ragService.js` and then integrated into `src/pages/api/ai/chat.js`.

*   **4.1. `src/services/ragService.js` - Core RAG Functions:**

    *   **`getEmbeddingForQuery(text, taskType)`:**
        *   Uses `@google/generative-ai`'s `models/text-embedding-004`.
        *   Accepts `text` to embed and `taskType` ("RETRIEVAL_DOCUMENT" for ingestion, "RETRIEVAL_QUERY" for search).

    *   **`fetchRagContext({ userId, originalUserQuery, limit = 3, numCandidates = 100 })`:**
        1.  **Embed User Query:**
            *   `const userQueryEmbedding = await getEmbeddingForQuery(originalUserQuery, "RETRIEVAL_QUERY");`
        2.  **Perform Vector Search:**
            *   Constructs a MongoDB aggregation pipeline using `$vectorSearch`.
            *   `const pipeline = [`
                *   `  {`
                *   `    $vectorSearch: {`
                *   `      index: process.env.VECTOR_SEARCH_INDEX_NAME || "vector_index_knowledge_cosine",`
                *   `      path: "embedding",`
                *   `      queryVector: userQueryEmbedding,`
                *   `      numCandidates: numCandidates,`
                *   `      limit: limit,`
                *   `      filter: { userId: new ObjectId(userId) } // Filter by the current user's ObjectId`
                *   `    }`
                *   `  },`
                *   `  {`
                *   `    $project: {`
                *   `      _id: 0, content: 1, sourceType: 1, originalFilename: 1, sourceUrl: 1, title: 1,`
                *   `      score: { $meta: "vectorSearchScore" } // Crucial for relevance assessment`
                *   `    }`
                *   `  }`
                *   `];`
            *   `const relevantDocs = await knowledgeDocumentsCollection.aggregate(pipeline).toArray();`
        3.  **Context Formatting:**
            *   If `relevantDocs` are found, their `content` is concatenated.
            *   `const context = relevantDocs.map(doc => \`Document (Source: \${doc.sourceType}, Title: \${doc.title || 'N/A'}\${doc.originalFilename ? ', File: ' + doc.originalFilename : ''}\${doc.sourceUrl ? ', URL: ' + doc.sourceUrl : ''}, Score: \${doc.score.toFixed(4)}):\\n\${doc.content}\`).join("\\n\\n---\\n\\n");`
        4.  **Return Value:** Returns an object `{ context: string | null, documents: Array<any> }`.

*   **4.2. RAG-Aware Tool Decision and Synthesis Flow (`chat.js` and `reactProcessorService.js`)**

    The system employs a sophisticated flow where RAG context informs the tool decision process within the ReAct agent.

    1.  **Initial Checks (`chat.js`):**
        *   User authentication, email verification, credit deduction.

    2.  **RAG Context Retrieval (`chat.js`):**
        *   `const { context: ragContextFromSearch, documents: ragDocuments } = await fetchRagContext({ userId: new ObjectId(user._id), originalUserQuery });`
        *   The `ragContextFromSearch` is a string containing concatenated relevant document chunks, and `ragDocuments` is an array of these documents including their scores.

    3.  **Relevance Check and `ragContextForReAct` Preparation (`chat.js`):**
        *   A `RELEVANCE_THRESHOLD` (e.g., 0.75, configurable via `process.env.RAG_RELEVANCE_THRESHOLD`) is used.
        *   `let ragContextForReAct = null;`
        *   `if (ragDocuments && ragDocuments.length > 0 && ragDocuments[0].score >= RELEVANCE_THRESHOLD) { ragContextForReAct = ragContextFromSearch; }`
        *   If the top RAG document's score meets or exceeds the threshold, `ragContextForReAct` (the string of context) is passed to the ReAct loop. Otherwise, `null` is passed.

    4.  **Invoke ReAct Loop with Potential RAG Context (`chat.js` calls `reactProcessorService.js`):**
        *   `chat.js` calls `executeInProcessReActLoop(originalUserQuery, ragContextForReAct, context)` in `reactProcessorService.js`.
        *   `originalUserQuery` is the user's raw query.
        *   `ragContextForReAct` is the RAG context string if deemed highly relevant, otherwise `null`.

    5.  **Tool Decision (`firstPassPrompt` in `reactProcessorService.js`):**
        *   `reactProcessorService.js` constructs a `firstPassPrompt` for the LLM.
        *   **Crucially, if `ragContextForReAct` was provided (i.e., not `null`), it is injected into this `firstPassPrompt`**, instructing the LLM to consider this knowledge base context *before* deciding whether to use a tool or select `tool_name: "none"`.
        *   The LLM responds with a JSON object: `{ "tool_name": "chosen_tool_or_none", "arguments": { ... } }`.

    6.  **Execution and Synthesis Path (`reactProcessorService.js`):**
        *   **A. If a specific tool is chosen (e.g., `search_engine`):**
            *   The tool is executed.
            *   The final answer is synthesized using the `originalUserQuery` and the `toolOutput` (see Section 5.4).
        *   **B. If `tool_name: "none"` is chosen by the LLM:**
            *   **B1. If `ragContextForReAct` was provided to `executeInProcessReActLoop`:**
                *   This means the LLM, after considering the RAG context in the `firstPassPrompt`, decided it was sufficient.
                *   A strict RAG synthesis prompt is used, combining `originalUserQuery` and the `ragContextForReAct` (see Section 5.2).
            *   **B2. If `ragContextForReAct` was `null` (not provided or not relevant enough):**
                *   This means the LLM decided its general knowledge was sufficient without needing tools or specific RAG.
                *   A general knowledge synthesis prompt is used with the `originalUserQuery` (see Section 5.3).

    7.  **Response Streaming:** The LLM's synthesized response is streamed back to the client.

**5. Prompting Strategies for Synthesis and Tool Decision**

This section details the key prompt structures used in `src/services/reactProcessorService.js`.

*   **5.1. Tool Decision Prompt (`firstPassPrompt`)**
    *   This prompt is dynamically constructed in `reactProcessorService.js` to guide the LLM in deciding whether a tool is needed, or if the (potentially provided) RAG context or its general knowledge is sufficient.
    *   **Full Template (from `reactProcessorService.js` lines 196-211):**
        ```text
        You are a helpful AI assistant with access to the following tools:
        ${toolDescriptions}

        IMPORTANT INSTRUCTIONS FOR TOOL USE:
        - Review any 'ADDITIONAL CONTEXT FROM KNOWLEDGE BASE' provided below before making a tool decision.
        - If the 'ADDITIONAL CONTEXT FROM KNOWLEDGE BASE' fully and accurately answers the 'User query', choose 'none' for the tool.
        - Otherwise, if your general knowledge is sufficient, choose 'none'.
        - Use tools like 'search_engine' for time-sensitive information (e.g., current prices, news), real-time data, or specifics not in your training or the provided context.
        {{#if ragContext}}

        ADDITIONAL CONTEXT FROM KNOWLEDGE BASE (Consider this before deciding on a tool):
        ---
        ${ragContext}
        ---
        {{/if}}

        User query: "${originalUserQuery}"

        Considering the user query, tools, instructions, and any additional context, do you need to use a tool? Or is the provided context/your general knowledge sufficient?
        Respond ONLY with a JSON object specifying the "tool_name" (e.g., "search_engine", or "none" if no tool is needed) and, if a tool is chosen, an "arguments" object for that tool. Example for tool use: {"tool_name": "search_engine", "arguments": {"query": "latest Bitcoin news"}}. Example for no tool: {"tool_name": "none"}.
        ```
    *   **Notes:**
        *   `${toolDescriptions}`: Dynamically generated string listing available tools.
        *   `{{#if ragContext}} ... {{/if}}`: This block, including "ADDITIONAL CONTEXT FROM KNOWLEDGE BASE" and `${ragContext}`, is conditionally included if `ragContext` was passed to `executeInProcessReActLoop`.
        *   `${ragContext}`: The RAG context string.
        *   `${originalUserQuery}`: The user's raw query.

*   **5.2. Strict RAG Synthesis Prompt (Tool: 'none', RAG Context Was Used for Decision)**
    *   Used when the LLM decides `tool_name: "none"` *and* `ragContext` was provided to `executeInProcessReActLoop` (meaning the LLM found the RAG context sufficient).
    *   **Full Template (from `reactProcessorService.js` lines 464-473):**
        ```text
        SYSTEM INSTRUCTION: You are in STRICT CONTEXT-ONLY MODE. Your primary goal is to answer the user's query using ONLY the "Context from Knowledge Base" provided. Do not use any external knowledge or your general training data. If the context does not contain the answer, explicitly state that the information is not available in the provided context.
        ROLE: You are hermitAI, an assistant that answers strictly from the provided text.
        Context from Knowledge Base:
        ---
        ${ragContext}
        ---
        Original Query:
        ${originalUserQuery}
        Your answer (ONLY from the context provided):
        ```
    *   **Notes:**
        *   `${ragContext}`: The RAG context that was initially passed to `executeInProcessReActLoop` and considered by the LLM.
        *   `${originalUserQuery}`: The user's original query.

*   **5.3. General Knowledge Synthesis Prompt (Tool: 'none', No RAG Context Used for Decision)**
    *   Used when the LLM decides `tool_name: "none"` *and* `ragContext` was *not* provided to `executeInProcessReActLoop` (or was `null`).
    *   **Full Template (from `reactProcessorService.js` lines 475-480):**
        ```text
        SYSTEM INSTRUCTION: You are hermitAI. Please provide a helpful and concise answer to the following user query based on your general knowledge.
        User Query:
        "${originalUserQuery}"

        Based on this, provide a direct answer.
        ```
    *   **Notes:**
        *   `${originalUserQuery}`: The user's original query.

*   **5.4. Post-Tool Synthesis Prompt**
    *   Used if a specific tool was chosen by the LLM and executed.
    *   **Template (from `reactProcessorService.js` lines 484-488, adjusted for clarity):**
        ```text
        User query: "${originalUserQuery}"
        I used the '${effectiveToolName}' tool and received the following information:
        ${JSON.stringify(toolOutput)}
        Based on this information, please provide a concise answer to the user's query. If the information is an error message, explain the error. If the information is complex, summarize it. Respond directly to the user.
        ```
    *   **Notes:**
        *   `${originalUserQuery}`: The user's original query.
        *   `${effectiveToolName}`: The name of the tool that was executed.
        *   `${JSON.stringify(toolOutput)}`: The output/results from the executed tool.

**6. Dependencies (Node.js)**

Reconfirming and adding necessary dependencies to `package.json`:

*   `mongodb`: (Already listed, official MongoDB driver)
*   `@google-cloud/aiplatform`: (For Vertex AI LLM calls, if still used for the chat model itself)
*   `@google/generative-ai`: (For Gemini embeddings - `models/text-embedding-004` and potentially Gemini chat models)
*   `pdf-parse`: (Already listed, for PDF text extraction)
*   `uuid`: For generating `chunkId`s (e.g., `npm install uuid` and `import { v4 as uuidv4 } from 'uuid';`)
*   Optional (Chunking):
    *   `langchain`: If using its text splitters. (This is a larger dependency).
    *   `recursive-character-text-splitter`: A more lightweight, focused library for this specific chunking strategy.
*   Optional (HTML Cleaning if `scrape_as_html` is used as fallback):
    *   `html-to-text`: For converting HTML from scraping to clean text.